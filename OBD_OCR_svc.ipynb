{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roYpaPfllE55"
   },
   "outputs": [],
   "source": [
    "# Install OCR Library\n",
    "pip install pytesseract opencv-python easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pt61RZCElhcs"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Set Tesseract executable path (Windows)\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Load YOLO model\n",
    "model = YOLO('runs/train/product_detect/weights/best.pt')\n",
    "\n",
    "# Load input image\n",
    "image = cv2.imread('test_image.jpg')\n",
    "\n",
    "# Run object detection\n",
    "results = model(image)\n",
    "\n",
    "# Run OCR on each detected bounding box region\n",
    "for box in results[0].boxes.xyxy:\n",
    "    x1, y1, x2, y2 = map(int, box)  # Extract coordinates\n",
    "    roi = image[y1:y2, x1:x2]       # Region of interest (ROI)\n",
    "\n",
    "    # Apply OCR\n",
    "    text = pytesseract.image_to_string(roi, lang='eng')\n",
    "    print(\"Detected Text:\", text)\n",
    "\n",
    "    # (Optional) Extract product name using keyword matching\n",
    "    if \"제품명\" in text:\n",
    "        print(f\"Detected Product Name: {text.strip()}\")\n",
    "\n",
    "# Visualize detection result\n",
    "cv2.imshow('Detected', image)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fz5JfbmvAgwM"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EONUCcsDZRMm"
   },
   "outputs": [],
   "source": [
    "!pip install google-cloud-vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJ3S8lTSXqC_"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from google.colab.patches import cv2_imshow\n",
    "from google.cloud import vision\n",
    "from google.cloud.vision_v1 import types\n",
    "import re\n",
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# NOTE: Secret credentials removed.\n",
    "# Set your Google Vision credentials before running, e.g.:\n",
    "# export GOOGLE_APPLICATION_CREDENTIALS=\"path/to/your_credentials.json\"\n",
    "# or in Colab:\n",
    "# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/your_credentials.json\"\n",
    "# -----------------------------\n",
    "\n",
    "# 1. Input image path\n",
    "image_path = '/content/test.jpg'\n",
    "\n",
    "# 2. Read image\n",
    "image = cv2.imread(image_path)\n",
    "if image is None:\n",
    "    print(\"Cannot read the image. Please check the file path.\")\n",
    "    exit()\n",
    "\n",
    "# 3. Preprocessing\n",
    "# (1) Convert to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# (2) Adaptive thresholding (binarization)\n",
    "adaptive_thresh = cv2.adaptiveThreshold(\n",
    "    gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n",
    ")\n",
    "\n",
    "# (3) Sharpening\n",
    "kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "sharpened = cv2.filter2D(adaptive_thresh, -1, kernel)\n",
    "\n",
    "# (4) Morphological operation\n",
    "kernel_morph = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 1))\n",
    "morphed = cv2.morphologyEx(sharpened, cv2.MORPH_CLOSE, kernel_morph)\n",
    "\n",
    "# 4. Run Google Vision OCR\n",
    "client = vision.ImageAnnotatorClient()\n",
    "\n",
    "# Read image as binary for Vision API\n",
    "with open(image_path, \"rb\") as image_file:\n",
    "    content = image_file.read()\n",
    "\n",
    "image_vision = types.Image(content=content)\n",
    "response = client.text_detection(image=image_vision)\n",
    "\n",
    "# OCR result\n",
    "texts = response.text_annotations\n",
    "\n",
    "# 5. Keyword-based extraction (tag / nutri / date)\n",
    "if texts:\n",
    "    print(\"OCR Results:\")\n",
    "    for text in texts:\n",
    "        detected_text = text.description\n",
    "\n",
    "        # tag class (example regex)\n",
    "        match_tag = re.search(r'명\\s+(\\S+)', detected_text)\n",
    "        if match_tag:\n",
    "            print(f\"Extracted text from TAG region: {match_tag.group(1)}\")\n",
    "\n",
    "        # nutri class (example regex)\n",
    "        match_nutri = re.search(r'영양정보(.*?)kcal', detected_text)\n",
    "        if match_nutri:\n",
    "            print(f\"Extracted text from NUTRI region: {match_nutri.group(1)}\")\n",
    "\n",
    "        # date class (example regex)\n",
    "        match_date = re.search(r'(24|25)(.*)', detected_text)\n",
    "        if match_date:\n",
    "            print(f\"Extracted text from DATE region: {match_date.group(2)}\")\n",
    "else:\n",
    "    print(\"No text detected.\")\n",
    "\n",
    "# Show original and preprocessed images\n",
    "print(\"Original image:\")\n",
    "cv2_imshow(image)\n",
    "\n",
    "print(\"Preprocessed image:\")\n",
    "cv2_imshow(morphed)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
